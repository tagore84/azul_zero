First:
AZUL_MACHINE_ID=mac caffeinate python3 scripts/train_azul.py \
  --n_games 100 \
  --simulations 200 \
  --epochs 20 \
  --eval_interval 10 \
  --eval_games 20 \
  --checkpoint_dir data/checkpoint_dir

Others:
AZUL_MACHINE_ID=mac caffeinate python3 scripts/train_azul.py \
  --n_games 100 \
  --simulations 200 \
  --epochs 20 \
  --eval_interval 25 \
  --eval_games 25 \
  --resume data/checkpoint_dir \
  --checkpoint_dir data/checkpoint_dir

Test:
AZUL_MACHINE_ID=mac python scripts/train_azul.py \
  --n_games 10 \
  --simulations 20 \
  --epochs 2 \
  --eval_interval 1 \
  --eval_games 4 \
  --checkpoint_dir data/checkpoint_dir_test \
  --log_dir logs/test_run


ToDo:
  1.	Entrenamiento con aprendizaje por refuerzo y MCTS para el juego de mesa Azul:
	  •	El agente utiliza MCTS con 200 simulaciones por movimiento para tomar decisiones durante las partidas.
	2.	Experience Replay:
	  •	Permite almacenar experiencias pasadas (estados, acciones, recompensas) en un búfer, y entrenar el modelo con muestras aleatorias de ese búfer.
	  •	Se vuelve más efectiva con el tiempo, al combinar experiencias de múltiples ejecuciones, enriqueciendo el entrenamiento y evitando el sobreajuste a experiencias recientes.
	3.	Entrenamiento iterativo vs. entrenamiento en bloque:
	  •	Entrenar el modelo de manera iterativa, es decir, en ciclos donde se generan partidas, se entrena el modelo, y se repite el proceso, suele ser más efectivo.
	  •	Esto permite retroalimentación continua, mejor uso de los datos, adaptación progresiva y detección temprana de problemas.
	4.	Tamaño del dataset y número de partidas:
	  •	Para un juego como Azul, un dataset inicial de decenas de miles de estados podría requerir alrededor de 100 a 500 partidas, mientras que un dataset más robusto (cientos de miles o millones de estados) podría requerir miles de partidas.
	5.	Estimación del tiempo de simulación:
	  •	Con 200 simulaciones por movimiento en un MacBook Pro, simular 5,000 partidas podría tardar alrededor de 5 días de ejecución continua.
	6.	Importancia del enfoque iterativo:
	•	Entrenar de forma iterativa suele ser más beneficioso que entrenar todo el modelo de una sola vez al final, ya que mejora la adaptabilidad del agente, permite ajustes graduales y optimiza el uso de los datos.



LOGS:
tensorboard --logdir logs


MERGE REPLAY BUFFER:
AZUL_MACHINE_ID=mac python scripts/merge_replay_buffers.py data/replay_buffer_merged.pkl \
    data/replay_buffer_mac.pkl data/replay_buffer_lg.pkl

CHOOSE BEST MODEL:
AZUL_MACHINE_ID=mac python scripts/select_best_model.py \
    data/checkpoint_latest_mac.pth \
    data/checkpoint_latest_lg.pth \
    data/checkpoint_best.pth

MERGE ALL:
AZUL_MACHINE_ID=mac python scripts/merge_all.py